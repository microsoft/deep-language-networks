{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec9b4e1-dcf5-42ed-af39-0ec56b90823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\"]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a651c2-9f82-4830-bd26-f53cbc47fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64723f-a113-466d-8057-920ce0d062c3",
   "metadata": {},
   "source": [
    "## Running from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1ccb88-ca67-4f86-b40e-506a5d4aac3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127673e8-188e-4726-959a-8b9c553f12f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the following sentence, then determine whether you return to the starting point.\n",
      "\n",
      "If you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "\n",
      "Answer:\n",
      "To solve this question, we need to keep track of the number of steps taken and the direction of each turn.\n",
      "\n",
      "Starting from the initial position, we take 9 steps forward. Then, we take another 9 steps forward. Next, we take 4 steps forward. Finally, we turn right.\n",
      "\n",
      "Since we have taken a total of 9 + 9 + 4 = 22 steps and turned right, we do not return to the starting point.\n",
      "\n",
      "\n",
      "Complete detailed textbook-level python code solutions\n",
      "```python\n",
      "# Initialize variables\n",
      "steps_taken = 0\n",
      "direction = 0  # 0: North, 1: East, 2: South, 3: West\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 4 steps forward\n",
      "steps_taken += 4\n",
      "\n",
      "# Turn right\n",
      "direction = (direction + 1) % 4\n",
      "\n",
      "# Check if returned to starting point\n",
      "if steps_taken == 0 and direction == 0:\n",
      "    print(\"Yes\")\n",
      "else:\n",
      "    print(\"No\")\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "generate_ids = model.generate(**inputs, max_length=500)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "_ = [print(o, \"\\n\") for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe8255e-66e2-450d-bf05-f3a6183719b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16e5f3b-3138-446f-9486-66426b69aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_instruction = (\n",
    "    \"Read the following sentence, then determine whether you return to the starting point.\"\n",
    ")\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    tokenizer_name_or_path=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf6382f-2825-471e-93fc-4c9fc17f2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5013a19-4ae9-4c9a-b989-0beec175a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/peft/peft_model.py:1180: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "generate_ids = peft_model.generate(**inputs, max_length=500)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0973e7a-d771-4027-afcc-1aa3bc1228f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the following sentence, then determine whether you return to the starting point.\n",
      "\n",
      "If you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "\n",
      "Answer:\n",
      "To solve this question, we need to keep track of the number of steps taken and the direction of each turn.\n",
      "\n",
      "Starting from the initial position, we take 9 steps forward. Then, we take another 9 steps forward. Finally, we take 4 steps forward.\n",
      "\n",
      "Since we have taken a total of 22 steps forward, we do not return to the starting point.\n",
      "\n",
      "\n",
      "Complete detailed textbook-level python code solutions\n",
      "```python\n",
      "# Initialize variables\n",
      "steps_taken = 0\n",
      "direction = \"forward\"\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 4 steps forward\n",
      "steps_taken += 4\n",
      "\n",
      "# Check if steps_taken is equal to 0\n",
      "if steps_taken == 0:\n",
      "    print(\"Yes\")\n",
      "else:\n",
      "    print(\"No\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8f0aab-b6aa-4ed8-b5d7-49e544b5bb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50256, 50256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573a3b96-6495-4bdc-9539-73acce089141",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = \"text\"\n",
    "label_column = \"label\"\n",
    "max_length = 128\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce281d5-b6c3-4bac-bf85-61042dc16028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "my_dict = {\"text\": sentences, \"label\": [\"No\"]}\n",
    "hf_dataset = Dataset.from_dict(my_dict)\n",
    "hf_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf9566ed-03c7-4581-9928-8f140d095c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, prefix, text_column, label_column, max_length):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{prefix}\\n\\n{x}\\n\\nAnswer:\\n\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        # concat the inputs and labels, mask the inputs part, and update the\n",
    "        # attention mask to match the new length (inputs + labels + pad_token_id)\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        # masks / ignores -100 tokens in the loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss\n",
    "        labels[\"input_ids\"][i] = [tokenizer.pad_token_id] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        # pad or truncate the batch to the specified max_length, and update the attention mask\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + model_inputs[\"attention_mask\"][i]\n",
    "        labels[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(\n",
    "            model_inputs[\"input_ids\"][i][:max_length]\n",
    "        )\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(\n",
    "            model_inputs[\"attention_mask\"][i][:max_length]\n",
    "        )\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0032626-d23b-46f1-952f-fefaee9dd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 154.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_datasets = hf_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=hf_dataset.column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"prefix\": initial_instruction,\n",
    "            \"text_column\": text_column,\n",
    "            \"label_column\": label_column,\n",
    "            \"max_length\": max_length,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a2e6e4-907c-490b-b479-b4c2112a6985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "518a968a-9f12-4cbe-aa78-cf5516dfb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "dataloader = accelerator.prepare(DataLoader(\n",
    "    processed_datasets,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c086d1b8-0f20-40ae-889c-d81155c9bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, tokenizer, device):\n",
    "    loss = 0\n",
    "    preds = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        print(outpus)\n",
    "        loss = outputs.loss\n",
    "        loss += loss.detach().float()\n",
    "        preds.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a21ddf7-c276-4567-be62-1c2a8608dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_one = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8968eb22-638e-44ce-bc60-d04ce17101f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256,  5569,   262,  1708,  6827,    11,\n",
       "            788,  5004,  1771,   345,  1441,   284,   262,  3599,   966,    13,\n",
       "            198,   198,  5569,   262,  1708,  6827,    11,   788,  5004,  1771,\n",
       "            345,  1441,   284,   262,  3599,   966,    13,   198,   198,  1532,\n",
       "            345,  1061,   777,  7729,    11,   466,   345,  1441,   284,   262,\n",
       "           3599,   966,    30,  7214,   860,  4831,    13,  7214,   860,  4831,\n",
       "             13,  7214,   604,  4831,    13,  6756,   826,    13,   198, 29046,\n",
       "             25,   198,    12,  3363,   198,    12,  1400,   198,   198, 33706,\n",
       "             25,   628,   198, 33706,    25,   198,  2949, 50256]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'labels': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256,  2949, 50256]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50f1e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_loss(generated_texts, target_texts):\n",
    "    losses = []\n",
    "    for generated_text, target_text in zip(generated_texts, target_texts):\n",
    "        generated_tokens = generated_text.split()\n",
    "        target_tokens = target_text.split()\n",
    "        loss = sum(generated_token != target_token for generated_token, target_token in zip(generated_tokens, target_tokens))\n",
    "        losses.append(loss)\n",
    "    \n",
    "    loss_tensor = torch.tensor(losses, dtype=torch.float32)\n",
    "    total_loss = torch.mean(loss_tensor)\n",
    "    \n",
    "    print(generated_texts)\n",
    "    print(target_texts)\n",
    "    print(total_loss.item())\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b8de9b3-725e-4f31-8734-0d01bd4a6bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Read the following sentence, then determine whether you return to the starting point.\\n\\nRead the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\\n\\nAnswer:\\nNo\\n\\n']\n",
      "['No']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "batch = {k: v.to(device) for k, v in batch_of_one.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**batch)\n",
    "    # input_texts = batch['input_ids']\n",
    "    # target_texts = batch['labels']\n",
    "    \n",
    "    # Tokenize input text\n",
    "    input_ids = batch['input_ids'] #tokenizer.batch_encode_plus(input_texts, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "\n",
    "    # Generate model output\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "    \n",
    "    # Decode generated output and target labels\n",
    "    target_ids = batch[\"labels\"] #tokenizer.batch_encode_plus(target_texts, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "    target_texts_decoded = [tokenizer.decode(target, skip_special_tokens=True) for target in target_ids]\n",
    "\n",
    "    loss = exact_match_loss(generated_texts, target_texts_decoded)\n",
    "    # optimizer.zero_grad()\n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "# loss = string_match_loss(generated_texts, target_texts_decoded)\n",
    "# loss += loss.detach().float()\n",
    "\n",
    "# ids = model.generate(**batch, max_length=500)\n",
    "# outputs = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "# print(outputs[0])\n",
    "\n",
    "# tokenizer.batch_decode(\n",
    "#     torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "#     skip_special_tokens=True,\n",
    "# )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dln",
   "language": "python",
   "name": "dln"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
