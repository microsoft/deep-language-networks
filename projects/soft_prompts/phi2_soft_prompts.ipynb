{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b64723f-a113-466d-8057-920ce0d062c3",
   "metadata": {},
   "source": [
    "## Soft-Prompts using Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1ccb88-ca67-4f86-b40e-506a5d4aac3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "sentences = [\"If you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "#inputs = tokenizer(sentences, return_tensors=\"pt\").to(device)\n",
    "#generate_ids = model.generate(**inputs, max_length=500, num_return_sequences=1, return_dict_in_generate=True)\n",
    "#outputs = tokenizer.batch_decode(generate_ids.sequences, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "#_ = [print(o, \"\\n\") for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe8255e-66e2-450d-bf05-f3a6183719b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "initial_instruction = (\n",
    "    \"Read the following question, then choose the correction answer.\"\n",
    ")\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    tokenizer_name_or_path=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9566ed-03c7-4581-9928-8f140d095c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, prefix, text_column, label_column, max_length):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{prefix}{x}\\n\\nAnswer:\\n\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        # Mask the inputs part, and update the attention mask to match the new length\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids\n",
    "        # masks / ignores -100 tokens in the loss\n",
    "        labels[\"input_ids\"][i] = [tokenizer.pad_token_id] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "        # pad or truncate the batch to the specified max_length, and update the attention mask\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + model_inputs[\"attention_mask\"][i]\n",
    "        labels[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(label_input_ids)\n",
    "        ) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(\n",
    "            model_inputs[\"input_ids\"][i][:max_length]\n",
    "        )\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(\n",
    "            model_inputs[\"attention_mask\"][i][:max_length]\n",
    "        )\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c28c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def convert_to_yes_no(generated_tokens):\n",
    "    return [\"Yes\" if TextBlob(token).sentiment.polarity > 0 else \"No\" for token in generated_tokens]\n",
    "\n",
    "def exact_match_loss(generated_texts, target_texts):\n",
    "    losses = []\n",
    "    for generated_text, target_text in zip(generated_texts, target_texts):\n",
    "        generated_texts_yes_no = convert_to_yes_no(generated_texts)\n",
    "        target_tokens = target_text.split()\n",
    "        loss = sum(generated_token != target_token for generated_token, target_token in zip(generated_texts_yes_no, target_tokens))\n",
    "        losses.append(loss)\n",
    "\n",
    "    loss_tensor = torch.tensor(losses, dtype=torch.float32)\n",
    "    total_loss = torch.mean(loss_tensor)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c086d1b8-0f20-40ae-889c-d81155c9bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, tokenizer, device, exact_match=True):\n",
    "    loss = 0\n",
    "    preds = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**batch, max_length=500, num_return_sequences=1, return_dict_in_generate=True) if exact_match else model(**batch)\n",
    "        \n",
    "        if exact_match:\n",
    "            generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs.sequences]        \n",
    "            target_texts_decoded = [tokenizer.decode(target, skip_special_tokens=True) for target in batch[\"labels\"]]\n",
    "\n",
    "        loss = exact_match_loss(generated_texts, target_texts_decoded) if exact_match else outputs.loss\n",
    "        loss += loss.detach().float()\n",
    "        labels = torch.where(batch['labels'] != -100, batch['labels'], tokenizer.pad_token_id)\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d765ea45-01e5-47d1-bca3-87a6929738db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dln.dataset import init_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "def load_dln_dataset_to_hf_dataset(dataset_id):\n",
    "    \"\"\"Some gynmastics to load the dln dataset into a HuggingFace Dataset.\n",
    "    dln.dataset should implement an interface compatible with HuggingFace\"\"\"\n",
    "\n",
    "    dln_dataset = init_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        seed=42,\n",
    "        data_dir=os.path.dirname(os.getcwd()) + \"/../data\",\n",
    "    )\n",
    "\n",
    "    def load_split(split):\n",
    "        text_data, label_data = dln_dataset.get_data(split)\n",
    "        data_dict = {\"text\": text_data, \"label\": label_data}\n",
    "        dataset = Dataset.from_dict(data_dict, split=split)\n",
    "        return dataset\n",
    "\n",
    "    # Combine the datasets into a DatasetDict\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": load_split(\"train\"),\n",
    "            \"dev\": load_split(\"dev\"),\n",
    "            \"test\": load_split(\"test\"),\n",
    "        }\n",
    "    )\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443a5190-5204-495e-bc5e-420857e89c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset from /home/chsingh/deep-language-networks/projects/../data/bbh ...\n",
      "we have 375 training, 375 dev, and 250 test data points.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 6187.92 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 6588.85 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 250/250 [00:00<00:00, 6185.01 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/peft/peft_model.py:1180: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "  6%|▋         | 1/16 [00:15<03:55, 15.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 2/16 [00:30<03:30, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|█▉        | 3/16 [00:38<02:36, 12.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 4/16 [00:53<02:36, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|███▏      | 5/16 [01:04<02:14, 12.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 6/16 [01:15<01:58, 11.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▍     | 7/16 [01:25<01:42, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 8/16 [01:40<01:39, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▋    | 9/16 [01:55<01:32, 13.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 10/16 [02:04<01:11, 12.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|██████▉   | 11/16 [02:19<01:04, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 12/16 [02:32<00:51, 12.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|████████▏ | 13/16 [02:44<00:38, 12.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 14/16 [02:59<00:26, 13.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|█████████▍| 15/16 [03:11<00:12, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 16/16 [03:23<00:00, 12.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training: init_test_ppl=tensor(1.0914) init_test_loss=tensor(0.0875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:07<00:56, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(34344.4844, device='cuda:0') train_epoch_loss=tensor(10.4442, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:14, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:07<00:56, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(34159.9609, device='cuda:0') train_epoch_loss=tensor(10.4388, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.37it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:07<00:56, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(34398.2422, device='cuda:0') train_epoch_loss=tensor(10.4458, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:47, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:06<00:56, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:09<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(34323.0039, device='cuda:0') train_epoch_loss=tensor(10.4436, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:00, 14.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:47, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:06<00:56, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(34509.7617, device='cuda:0') train_epoch_loss=tensor(10.4490, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:06<00:56, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=tensor(35165.0742, device='cuda:0') train_epoch_loss=tensor(10.4678, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:06<00:56, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=tensor(35310.8594, device='cuda:0') train_epoch_loss=tensor(10.4719, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:06<00:56, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl=tensor(34515.0273, device='cuda:0') train_epoch_loss=tensor(10.4492, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:06<00:56, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl=tensor(32950.2422, device='cuda:0') train_epoch_loss=tensor(10.4028, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.36it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 1/24 [00:14<05:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 2/24 [00:28<05:07, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 3/24 [00:43<05:01, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 4/24 [00:53<04:13, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 5/24 [01:07<04:09, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 6/24 [01:21<04:06, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 7/24 [01:36<03:58, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 8/24 [01:51<03:48, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 9/24 [02:05<03:33, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 10/24 [02:16<03:07, 13.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 11/24 [02:31<02:59, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 12/24 [02:46<02:48, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 13/24 [03:01<02:37, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 14/24 [03:09<02:04, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 15/24 [03:20<01:49, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 16/24 [03:31<01:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 17/24 [03:46<01:28, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 18/24 [03:57<01:13, 12.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 19/24 [04:07<00:56, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 20/24 [04:18<00:45, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 21/24 [04:29<00:34, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 22/24 [04:44<00:24, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 23/24 [04:59<00:13, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 24/24 [05:10<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl=tensor(34089.8281, device='cuda:0') train_epoch_loss=tensor(10.4368, device='cuda:0') eval_ppl=tensor(1.0241) eval_epoch_loss=tensor(0.0238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|▋         | 1/16 [00:14<03:41, 14.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▎        | 2/16 [00:29<03:26, 14.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|█▉        | 3/16 [00:38<02:34, 11.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 4/16 [00:52<02:36, 13.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|███▏      | 5/16 [01:03<02:14, 12.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 6/16 [01:14<01:58, 11.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▍     | 7/16 [01:25<01:42, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 8/16 [01:39<01:39, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▋    | 9/16 [01:54<01:32, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▎   | 10/16 [02:04<01:12, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|██████▉   | 11/16 [02:18<01:04, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 12/16 [02:31<00:51, 12.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|████████▏ | 13/16 [02:44<00:38, 12.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 14/16 [02:59<00:26, 13.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|█████████▍| 15/16 [03:10<00:12, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 16/16 [03:22<00:00, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training: init_test_ppl=tensor(1.0914) init_test_loss=tensor(0.0875)\n",
      "Test after training: final_test_ppl=tensor(1.0914) final_test_loss=tensor(0.0875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model_name_or_path = \"microsoft/phi-2\"\n",
    "    tokenizer_name_or_path = \"microsoft/phi-2\"\n",
    "\n",
    "    dataset_id = \"navigate\"\n",
    "    initial_instruction = (\n",
    "        \"Read the following question, then choose the correct answer.\"\n",
    "    )\n",
    "    text_column = \"text\"\n",
    "    label_column = \"label\"\n",
    "    max_length = 128\n",
    "    lr = 3e-2\n",
    "    num_epochs = 10\n",
    "    batch_size = 16\n",
    "\n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=8,\n",
    "        prompt_tuning_init_text=initial_instruction,\n",
    "        tokenizer_name_or_path=model_name_or_path,\n",
    "    )\n",
    "\n",
    "    dataset = load_dln_dataset_to_hf_dataset(dataset_id)\n",
    "\n",
    "    classes = list(set(dataset[\"train\"][\"label\"]))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, device_map=\"auto\", padding_side='left')\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    target_max_length = max(\n",
    "        [len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes]\n",
    "    )\n",
    "    print(target_max_length)\n",
    "\n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"prefix\": '', #initial_instruction + \"\\n\\n\",\n",
    "            \"text_column\": text_column,\n",
    "            \"label_column\": label_column,\n",
    "            \"max_length\": max_length,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"dev\"]\n",
    "    test_dataset = processed_datasets[\"test\"]\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Send everything through `accelerator.prepare`\n",
    "    train_loader, eval_loader, test_loader, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, eval_dataloader, test_dataloader, model, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    init_test_loss = test(test_dataloader, model, tokenizer, device)\n",
    "    init_test_ppl = torch.exp(init_test_loss)  # Perplexity\n",
    "    print(f\"Test before training: {init_test_ppl=} {init_test_loss=}\")\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            inputs = tokenizer(sentences, return_tensors=\"pt\").to(device)\n",
    "            output = model(**batch) #, max_length=1, num_return_sequences=1, return_dict_in_generate=True)\n",
    "\n",
    "            # generated_texts = [tokenizer.decode(out[-1], skip_special_tokens=True) for out in output.sequences] #tokenizer.batch_decode(output.sequences, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            # target_texts_decoded = [tokenizer.decode(target, skip_special_tokens=True) for target in batch[\"labels\"]]\n",
    "\n",
    "            loss = output.loss; #exact_match_loss(generated_texts, target_texts_decoded)\n",
    "            loss.requires_grad_(True)\n",
    "\n",
    "            total_loss += loss.detach().float()\n",
    "            optimizer.zero_grad()\n",
    "            #accelerator.backward(output.loss)\n",
    "            #optimizer.step()\n",
    "            #lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        eval_epoch_loss = test(eval_dataloader, model, tokenizer, device)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        print(\n",
    "            f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\"\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    final_test_loss = test(test_dataloader, model, tokenizer, device)\n",
    "    final_test_ppl = torch.exp(final_test_loss)\n",
    "    print(f\"Test before training: {init_test_ppl=} {init_test_loss=}\")\n",
    "    print(f\"Test after training: {final_test_ppl=} {final_test_loss=}\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
