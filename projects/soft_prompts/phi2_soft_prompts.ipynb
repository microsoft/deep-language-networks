{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec9b4e1-dcf5-42ed-af39-0ec56b90823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\"]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a651c2-9f82-4830-bd26-f53cbc47fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64723f-a113-466d-8057-920ce0d062c3",
   "metadata": {},
   "source": [
    "## Running from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1ccb88-ca67-4f86-b40e-506a5d4aac3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127673e8-188e-4726-959a-8b9c553f12f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the following sentence, then determine whether you return to the starting point.\n",
      "\n",
      "If you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "\n",
      "Answer:\n",
      "To solve this question, we need to keep track of the number of steps taken and the direction of each turn.\n",
      "\n",
      "Starting from the initial position, we take 9 steps forward. Then, we take another 9 steps forward. Next, we take 4 steps forward. Finally, we turn right.\n",
      "\n",
      "Since we have taken a total of 9 + 9 + 4 = 22 steps and turned right, we do not return to the starting point.\n",
      "\n",
      "\n",
      "Complete detailed textbook-level python code solutions\n",
      "```python\n",
      "# Initialize variables\n",
      "steps_taken = 0\n",
      "direction = 0  # 0: North, 1: East, 2: South, 3: West\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 4 steps forward\n",
      "steps_taken += 4\n",
      "\n",
      "# Turn right\n",
      "direction = (direction + 1) % 4\n",
      "\n",
      "# Check if returned to starting point\n",
      "if steps_taken == 0 and direction == 0:\n",
      "    print(\"Yes\")\n",
      "else:\n",
      "    print(\"No\")\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "generate_ids = model.generate(**inputs, max_length=500)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "_ = [print(o, \"\\n\") for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe8255e-66e2-450d-bf05-f3a6183719b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16e5f3b-3138-446f-9486-66426b69aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_instruction = (\n",
    "    \"Read the following sentence, then determine whether you return to the starting point.\"\n",
    ")\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    tokenizer_name_or_path=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf6382f-2825-471e-93fc-4c9fc17f2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5013a19-4ae9-4c9a-b989-0beec175a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/peft/peft_model.py:1180: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "generate_ids = peft_model.generate(**inputs, max_length=500)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0973e7a-d771-4027-afcc-1aa3bc1228f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the following sentence, then determine whether you return to the starting point.\n",
      "\n",
      "If you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "\n",
      "Answer:\n",
      "To solve this question, we need to keep track of the number of steps taken and the direction of each turn.\n",
      "\n",
      "Starting from the initial position, we take 9 steps forward. Then, we take another 9 steps forward. Finally, we take 4 steps forward.\n",
      "\n",
      "Since we have taken a total of 22 steps forward, we do not return to the starting point.\n",
      "\n",
      "\n",
      "Complete detailed textbook-level python code solutions\n",
      "```python\n",
      "# Initialize variables\n",
      "steps_taken = 0\n",
      "direction = \"forward\"\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 4 steps forward\n",
      "steps_taken += 4\n",
      "\n",
      "# Check if steps_taken is equal to 0\n",
      "if steps_taken == 0:\n",
      "    print(\"Yes\")\n",
      "else:\n",
      "    print(\"No\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8f0aab-b6aa-4ed8-b5d7-49e544b5bb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50256, 50256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573a3b96-6495-4bdc-9539-73acce089141",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = \"text\"\n",
    "label_column = \"label\"\n",
    "max_length = 128\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce281d5-b6c3-4bac-bf85-61042dc16028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "my_dict = {\"text\": sentences, \"label\": [\"No\"]}\n",
    "hf_dataset = Dataset.from_dict(my_dict)\n",
    "hf_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9566ed-03c7-4581-9928-8f140d095c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, prefix, text_column, label_column, max_length):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{prefix}\\n\\n{x}\\n\\nAnswer:\\n\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        # concat the inputs and labels, mask the inputs part, and update the\n",
    "        # attention mask to match the new length (inputs + labels + pad_token_id)\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        # masks / ignores -100 tokens in the loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss\n",
    "        labels[\"input_ids\"][i] = [tokenizer.pad_token_id] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        # pad or truncate the batch to the specified max_length, and update the attention mask\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + model_inputs[\"attention_mask\"][i]\n",
    "        labels[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(\n",
    "            model_inputs[\"input_ids\"][i][:max_length]\n",
    "        )\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(\n",
    "            model_inputs[\"attention_mask\"][i][:max_length]\n",
    "        )\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0032626-d23b-46f1-952f-fefaee9dd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 268.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_datasets = hf_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=hf_dataset.column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"prefix\": initial_instruction,\n",
    "            \"text_column\": text_column,\n",
    "            \"label_column\": label_column,\n",
    "            \"max_length\": max_length,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a2e6e4-907c-490b-b479-b4c2112a6985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518a968a-9f12-4cbe-aa78-cf5516dfb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "dataloader = accelerator.prepare(DataLoader(\n",
    "    processed_datasets,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c086d1b8-0f20-40ae-889c-d81155c9bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, tokenizer, device, exact_match=False):\n",
    "    loss = 0\n",
    "    preds = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(batch[\"input_ids\"], max_length=500, num_return_sequences=1) if exact_match else model(**batch)\n",
    "        \n",
    "        if exact_match:\n",
    "            generated_texts = tokenizer.batch_decode(outputs,  skip_special_tokens=True) #[tokenizer.decode(out, skip_special_tokens=True) for out in outputs]        \n",
    "            target_texts_decoded = [tokenizer.decode(target, skip_special_tokens=True) for target in batch[\"labels\"]]\n",
    "\n",
    "        loss = exact_match_loss(generated_texts, target_texts_decoded) if exact_match else outputs.loss\n",
    "        loss += loss.detach().float()\n",
    "        # preds.extend(\n",
    "        #     tokenizer.batch_decode(\n",
    "        #         torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "        #         skip_special_tokens=True,\n",
    "        #     )\n",
    "        # )\n",
    "        labels = torch.where(batch['labels'] != -100, batch['labels'], tokenizer.pad_token_id)\n",
    "\n",
    "        # targets = []\n",
    "        # for label_row in labels:\n",
    "        #     decoded_tokens = tokenizer.convert_ids_to_tokens(label_row, skip_special_tokens=True)\n",
    "        #     decoded_text = tokenizer.convert_tokens_to_string(decoded_tokens)\n",
    "        #     targets.append(decoded_text)\n",
    "\n",
    "        # if (exact_match):\n",
    "        #     print(preds)\n",
    "            # print(targets)\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a21ddf7-c276-4567-be62-1c2a8608dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_one = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8968eb22-638e-44ce-bc60-d04ce17101f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256,  5569,   262,  1708,  6827,    11,\n",
       "            788,  5004,  1771,   345,  1441,   284,   262,  3599,   966,    13,\n",
       "            198,   198,  5569,   262,  1708,  6827,    11,   788,  5004,  1771,\n",
       "            345,  1441,   284,   262,  3599,   966,    13,   198,   198,  1532,\n",
       "            345,  1061,   777,  7729,    11,   466,   345,  1441,   284,   262,\n",
       "           3599,   966,    30,  7214,   860,  4831,    13,  7214,   860,  4831,\n",
       "             13,  7214,   604,  4831,    13,  6756,   826,    13,   198, 29046,\n",
       "             25,   198,    12,  3363,   198,    12,  1400,   198,   198, 33706,\n",
       "             25,   628,   198, 33706,    25,   198,  2949, 50256]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'labels': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256,  2949, 50256]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50f1e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_loss(generated_texts, target_texts):\n",
    "    losses = []\n",
    "    for generated_text, target_text in zip(generated_texts, target_texts):\n",
    "        generated_tokens = generated_text.split()\n",
    "        target_tokens = target_text.split()\n",
    "        loss = sum(generated_token != target_token for generated_token, target_token in zip(generated_tokens, target_tokens))\n",
    "        losses.append(loss)\n",
    "    \n",
    "    loss_tensor = torch.tensor(losses, dtype=torch.float32)\n",
    "    total_loss = torch.mean(loss_tensor)\n",
    "    \n",
    "    print(generated_texts)\n",
    "    print(target_texts)\n",
    "    print(total_loss.item())\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b8de9b3-725e-4f31-8734-0d01bd4a6bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 128, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Read the following sentence, then determine whether you return to the starting point.\\n\\nRead the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\\n\\nAnswer:\\nNo\\n\\n']\n",
      "['No']\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chsingh/anaconda3/envs/dln/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 128, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = {k: v.to(device) for k, v in batch_of_one.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**batch)\n",
    "    # input_texts = batch['input_ids']\n",
    "    # target_texts = batch['labels']\n",
    "    \n",
    "    # Tokenize input text\n",
    "    input_ids = batch['input_ids'] #tokenizer.batch_encode_plus(input_texts, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "\n",
    "    # Generate model output\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "    \n",
    "    # Decode generated output and target labels\n",
    "    target_ids = batch[\"labels\"] #tokenizer.batch_encode_plus(target_texts, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "    target_texts_decoded = [tokenizer.decode(target, skip_special_tokens=True) for target in target_ids]\n",
    "\n",
    "    loss = exact_match_loss(generated_texts, target_texts_decoded)\n",
    "    # optimizer.zero_grad()\n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "# loss = string_match_loss(generated_texts, target_texts_decoded)\n",
    "# loss += loss.detach().float()\n",
    "\n",
    "# ids = model.generate(**batch, max_length=500)\n",
    "# outputs = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "# print(outputs[0])\n",
    "\n",
    "# tokenizer.batch_decode(\n",
    "#     torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "#     skip_special_tokens=True,\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d765ea45-01e5-47d1-bca3-87a6929738db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dln.dataset import init_dataset\n",
    "def load_dln_dataset_to_hf_dataset(dataset_id):\n",
    "    \"\"\"Some gynmastics to load the dln dataset into a HuggingFace Dataset.\n",
    "    dln.dataset should implement an interface compatible with HuggingFace\"\"\"\n",
    "\n",
    "    dln_dataset = init_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        seed=42,\n",
    "        data_dir=os.path.dirname(os.getcwd()) + \"/../data\",\n",
    "    )\n",
    "\n",
    "    def load_split(split):\n",
    "        text_data, label_data = dln_dataset.get_data(split)\n",
    "        data_dict = {\"text\": text_data, \"label\": label_data}\n",
    "        dataset = Dataset.from_dict(data_dict, split=split)\n",
    "        return dataset\n",
    "\n",
    "    # Combine the datasets into a DatasetDict\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": load_split(\"train\"),\n",
    "            \"dev\": load_split(\"dev\"),\n",
    "            \"test\": load_split(\"test\"),\n",
    "        }\n",
    "    )\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "443a5190-5204-495e-bc5e-420857e89c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "accelerator = Accelerator()\n",
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "def train(rank, world_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
    "\n",
    "    model_name_or_path = \"microsoft/phi-2\"\n",
    "    tokenizer_name_or_path = \"microsoft/phi-2\"\n",
    "\n",
    "    dataset_id = \"navigate\"\n",
    "    initial_instruction = (\n",
    "        \"Read the following sentence, then determine whether you return to the starting point.\"\n",
    "    )\n",
    "    text_column = \"text\"\n",
    "    label_column = \"label\"\n",
    "    max_length = 128\n",
    "    lr = 3e-2\n",
    "    num_epochs = 10\n",
    "    # batch_size = 8\n",
    "    batch_size = 16\n",
    "\n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=8,\n",
    "        prompt_tuning_init_text=initial_instruction,\n",
    "        tokenizer_name_or_path=model_name_or_path,\n",
    "    )\n",
    "\n",
    "    dataset = load_dln_dataset_to_hf_dataset(dataset_id)\n",
    "\n",
    "    classes = list(set(dataset[\"train\"][\"label\"]))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, device_map=\"auto\")\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    target_max_length = max(\n",
    "        [len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes]\n",
    "    )\n",
    "    print(target_max_length)\n",
    "\n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"prefix\": initial_instruction,\n",
    "            \"text_column\": text_column,\n",
    "            \"label_column\": label_column,\n",
    "            \"max_length\": max_length,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"dev\"]\n",
    "    test_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(model.print_trainable_parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[accelerator.device]).to(device)\n",
    "        \n",
    "    # Send everything through `accelerator.prepare`\n",
    "    train_loader, eval_loader, test_loader, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, eval_dataloader, test_dataloader, model, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    init_test_loss = test(test_dataloader, model, tokenizer, device)\n",
    "    init_test_ppl = torch.exp(init_test_loss)  # Perplexity\n",
    "    print(f\"Test before training: {init_test_ppl=} {init_test_loss=}\")\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            output = model.module.generate(batch[\"input_ids\"], max_length=500, num_return_sequences=1)\n",
    "\n",
    "            generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in output]    \n",
    "            target_texts_decoded = [tokenizer.decode(target, skip_special_tokens=True) for target in batch[\"labels\"]]\n",
    "\n",
    "            loss = exact_match_loss(generated_texts, target_texts_decoded)\n",
    "            # optimizer.zero_grad()\n",
    "            loss.requires_grad_(True)\n",
    "\n",
    "            # loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        eval_epoch_loss = test(eval_dataloader, model, tokenizer, device)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        print(\n",
    "            f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\"\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    final_test_loss = test(test_dataloader, model, tokenizer, device)\n",
    "    final_test_ppl = torch.exp(final_test_loss)\n",
    "    print(f\"Test before training: {init_test_ppl=} {init_test_loss=}\")\n",
    "    print(f\"Test after training: {final_test_ppl=} {final_test_loss=}\")\n",
    "\n",
    "    # model.module.save_pretrained(\"data/models/\" + model_name_or_path)\n",
    "\n",
    "    # config = PeftConfig.from_pretrained(\"data/models/\" + model_name_or_path)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "    # lora_model = PeftModel.from_pretrained(model, \"data/models/\" + model_name_or_path)\n",
    "    # lora_model.to(device)\n",
    "\n",
    "    # final_test_loss = test(test_dataloader, lora_model, tokenizer, device, True)\n",
    "    # final_test_ppl = torch.exp(final_test_loss)\n",
    "\n",
    "    # print(f\"Test after loading: {final_test_ppl=} {final_test_loss=}\")\n",
    "\n",
    "    sentences = [\"Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\"]\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "    generate_ids = model.generate(**inputs, max_length=500)\n",
    "    outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print([print(o, \"\\n\") for o in outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efd9f281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset from /home/chsingh/deep-language-networks/projects/../data/bbh ...\n",
      "loaded dataset from /home/chsingh/deep-language-networks/projects/../data/bbh ...we have 375 training, 375 dev, and 250 test data points.\n",
      "\n",
      "we have 375 training, 375 dev, and 250 test data points.\n",
      "loaded dataset from /home/chsingh/deep-language-networks/projects/../data/bbh ...\n",
      "we have 375 training, 375 dev, and 250 test data points.\n",
      "loaded dataset from /home/chsingh/deep-language-networks/projects/../data/bbh ...\n",
      "we have 375 training, 375 dev, and 250 test data points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Running tokenizer on dataset:   0%|          | 0/375 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/375 [00:00<?, ? examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/375 [00:00<?, ? examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4101.81 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4093.11 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4078.31 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4078.51 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4110.65 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4149.89 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4156.00 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 250/250 [00:00<00:00, 4101.82 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 250/250 [00:00<00:00, 4130.90 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 4142.98 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 250/250 [00:00<00:00, 4124.18 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 250/250 [00:00<00:00, 4097.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Process-1\n",
      "Done with Process-2\n",
      "Done with Process-3\n",
      "Done with Process-4\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12346'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '4'\n",
    "\n",
    "world_size = 4  # Number of GPUs\n",
    "processes = []\n",
    "for rank in range(world_size):\n",
    "    p = Process(target=train, args=(rank, world_size))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "    print(\"Done with \" + p.name)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
