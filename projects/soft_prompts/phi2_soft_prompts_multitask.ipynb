{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b64723f-a113-466d-8057-920ce0d062c3",
   "metadata": {},
   "source": [
    "## Multitask prompt tuning using Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1ccb88-ca67-4f86-b40e-506a5d4aac3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe8255e-66e2-450d-bf05-f3a6183719b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff25dd9553e4ea1b3a767e062d72cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    MultitaskPromptTuningConfig,\n",
    "    MultitaskPromptTuningInit,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "initial_instruction = (\n",
    "    \"Read the following question, then choose the correction answer.\"\n",
    ")\n",
    "\n",
    "peft_config = MultitaskPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_tasks=2,\n",
    "    prompt_tuning_init=MultitaskPromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    num_transformer_submodules=1,\n",
    "    tokenizer_name_or_path=model_id,\n",
    ")\n",
    "\n",
    "model = None\n",
    "saved_model1 = None\n",
    "saved_model2 = None\n",
    "\n",
    "try:\n",
    "    sentences = [\"Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\"]\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model.to(device)\n",
    "    generate_ids = model.generate(**inputs, max_length=500)\n",
    "    outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    #print(outputs[0])\n",
    "\n",
    "    print(\"Using saved model from data/models/\" + model_id)\n",
    "    saved_model1 = PeftModel.from_pretrained(model, \"data/models/\" + model_id + \"/model1\")\n",
    "    saved_model2 = PeftModel.from_pretrained(model, \"data/models/\" + model_id + \"/model2\")\n",
    "    saved_model1.to(device)\n",
    "    saved_model2.to(device)\n",
    "    task_ids = [0 for i in inputs[\"input_ids\"]]\n",
    "    task_ids = torch.tensor(task_ids).to(device)\n",
    "    generate_ids1 = saved_model1.generate(**inputs, max_length=500, task_ids=task_ids)\n",
    "    task_ids = [1 for i in inputs[\"input_ids\"]]\n",
    "    task_ids = torch.tensor(task_ids).to(device)\n",
    "    generate_ids2 = saved_model2.generate(**inputs, max_length=500, task_ids=task_ids)\n",
    "    outputs1 = tokenizer.batch_decode(generate_ids1, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    outputs2 = tokenizer.batch_decode(generate_ids2, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    #print(outputs1[0])\n",
    "    #print(outputs2[0])\n",
    "except ValueError:\n",
    "    print(\"Model not found, training new model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9566ed-03c7-4581-9928-8f140d095c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, prefix, text_column, label_column, max_length):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{prefix}{x}\\n\\nAnswer:\\n\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=max_length)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Replace padding tokens in the labels with -100\n",
    "    labels[\"input_ids\"] = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels[\"input_ids\"]]\n",
    "\n",
    "    task_ids = [0 for i in labels[\"input_ids\"]]\n",
    "    task_ids = torch.tensor(task_ids)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"task_ids\"] = task_ids\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6704297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobs_for_classes(output_logits, classes):\n",
    "    logits = [0 for _ in range(len(classes))]\n",
    "    for i, target in enumerate(classes):\n",
    "        expanded_classes = [target] + [f\" {target}\"] + [f\"{target.lower()}\"] + [f\" {target.lower()}\"]\n",
    "        encoded_classes = [tokenizer.encode(c, return_tensors=\"pt\", padding=True).to(device) for c in expanded_classes]\n",
    "        for token in encoded_classes:\n",
    "            logits[i] += output_logits[token]\n",
    "    return F.log_softmax(torch.tensor(logits), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c28c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_loss(outputs, labels):     \n",
    "    target_texts = [tokenizer.decode([tok for tok in target if tok != -100], skip_special_tokens=True) for target in labels]\n",
    "    targets = list(set(target_texts))\n",
    "    generated_texts = [targets[np.argmax(logprobs_for_classes(out[-1], targets))] for out in outputs.logits]        \n",
    "\n",
    "    losses = []\n",
    "    for generated_text, target_text in zip(generated_texts, target_texts):\n",
    "        generated_tokens = generated_text.split()\n",
    "        target_tokens = target_text.split()\n",
    "        loss = sum(generated_token != target_token for generated_token, target_token in zip(generated_tokens, target_tokens))\n",
    "        losses.append(loss)\n",
    "\n",
    "    loss_tensor = torch.tensor(losses, dtype=torch.float32)\n",
    "    total_loss = torch.mean(loss_tensor)\n",
    "    return total_loss, generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c086d1b8-0f20-40ae-889c-d81155c9bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model1, model2, tokenizer, device, exact_match=True):\n",
    "    total_loss = 0\n",
    "    test_preds = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output1 = model1(**batch, output_hidden_states=True)\n",
    "        inputs_embeds = output1.hidden_states[-1]\n",
    "        sequence_length = inputs_embeds.shape[1]\n",
    "        labels = batch['labels']\n",
    "        attention_mask = torch.ones(inputs_embeds.shape[:2], device=device)\n",
    "        padding = torch.full((labels.shape[0], sequence_length - labels.shape[1]), -100, dtype=labels.dtype, device=labels.device)\n",
    "        labels = torch.cat([padding, labels], dim=1).to(device)\n",
    "        task_ids = torch.tensor([1 for i in batch[\"task_ids\"]]).to(device)\n",
    "        output2 = model2(inputs_embeds=inputs_embeds, labels=labels, task_ids=task_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        \n",
    "        loss, preds = exact_match_loss(output2, batch[\"labels\"]) if exact_match else (output2.loss, [])\n",
    "        total_loss += loss.detach().float()\n",
    "        test_preds.extend(preds)\n",
    "\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_loss, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d765ea45-01e5-47d1-bca3-87a6929738db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dln.dataset import init_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def load_dln_dataset_to_hf_dataset(dataset_id):\n",
    "    \"\"\"Some gynmastics to load the dln dataset into a HuggingFace Dataset.\n",
    "    dln.dataset should implement an interface compatible with HuggingFace\"\"\"\n",
    "\n",
    "    dln_dataset = init_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        seed=42,\n",
    "        data_dir=os.path.dirname(os.getcwd()) + \"/../data\",\n",
    "    )\n",
    "\n",
    "    def load_split(split):\n",
    "        text_data, label_data = dln_dataset.get_data(split)\n",
    "        data_dict = {\"text\": text_data, \"label\": label_data}\n",
    "        dataset = Dataset.from_dict(data_dict, split=split)\n",
    "        return dataset\n",
    "\n",
    "    # Combine the datasets into a DatasetDict\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": load_split(\"train\"),\n",
    "            \"dev\": load_split(\"dev\"),\n",
    "            \"test\": load_split(\"test\"),\n",
    "        }\n",
    "    )\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443a5190-5204-495e-bc5e-420857e89c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset from /home/chsingh/deep-language-networks/projects/../data/bbh ...\n",
      "we have 375 training, 375 dev, and 250 test data points.\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88b98b6372b4705a5e153cbfc597fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chsingh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6044393d339b42a399d0a69a85bdae85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42729a038dfd426082fd8be3f17ba1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 32/32 [00:24<00:00,  1.30it/s]\n",
      "100%|██████████| 32/32 [00:24<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training1: init_test_ppl1=tensor(1.5130) init_test_loss1=tensor(0.4141)\n",
      "Test before training2: init_test_ppl2=tensor(1.5130) init_test_loss2=tensor(0.4141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  1.98s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl1=tensor(61.4932) train_epoch_loss1=4.118926109151637 eval_ppl1=tensor(2.3009, device='cuda:0') eval_epoch_loss1=tensor(0.8333, device='cuda:0')\n",
      "epoch=0: train_ppl2=tensor(6.9223) train_epoch_loss2=1.9347492824209498 eval_ppl2=tensor(2.3009, device='cuda:0') eval_epoch_loss2=tensor(0.8333, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl1=tensor(2.1528) train_epoch_loss1=0.7667878523786017 eval_ppl1=tensor(2.0596, device='cuda:0') eval_epoch_loss1=tensor(0.7225, device='cuda:0')\n",
      "epoch=1: train_ppl2=tensor(2.2656) train_epoch_loss2=0.8178455943756915 eval_ppl2=tensor(2.0596, device='cuda:0') eval_epoch_loss2=tensor(0.7225, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl1=tensor(2.0217) train_epoch_loss1=0.7039550219444518 eval_ppl1=tensor(2.0083, device='cuda:0') eval_epoch_loss1=tensor(0.6973, device='cuda:0')\n",
      "epoch=2: train_ppl2=tensor(2.1115) train_epoch_loss2=0.7473819610920358 eval_ppl2=tensor(2.0083, device='cuda:0') eval_epoch_loss2=tensor(0.6973, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl1=tensor(1.9681) train_epoch_loss1=0.6770760493075594 eval_ppl1=tensor(2.1537, device='cuda:0') eval_epoch_loss1=tensor(0.7672, device='cuda:0')\n",
      "epoch=3: train_ppl2=tensor(2.0613) train_epoch_loss2=0.7233324875222876 eval_ppl2=tensor(2.1537, device='cuda:0') eval_epoch_loss2=tensor(0.7672, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl1=tensor(1.9851) train_epoch_loss1=0.6856874937706805 eval_ppl1=tensor(1.9317, device='cuda:0') eval_epoch_loss1=tensor(0.6584, device='cuda:0')\n",
      "epoch=4: train_ppl2=tensor(2.0836) train_epoch_loss2=0.7340930281801427 eval_ppl2=tensor(1.9317, device='cuda:0') eval_epoch_loss2=tensor(0.6584, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl1=tensor(1.9839) train_epoch_loss1=0.6850424833754276 eval_ppl1=tensor(1.9393, device='cuda:0') eval_epoch_loss1=tensor(0.6623, device='cuda:0')\n",
      "epoch=5: train_ppl2=tensor(2.0896) train_epoch_loss2=0.7369965416319827 eval_ppl2=tensor(1.9393, device='cuda:0') eval_epoch_loss2=tensor(0.6623, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl1=tensor(1.8865) train_epoch_loss1=0.634727843898408 eval_ppl1=tensor(2.2600, device='cuda:0') eval_epoch_loss1=tensor(0.8154, device='cuda:0')\n",
      "epoch=6: train_ppl2=tensor(2.0496) train_epoch_loss2=0.7176343697182676 eval_ppl2=tensor(2.2600, device='cuda:0') eval_epoch_loss2=tensor(0.8154, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl1=tensor(1.8684) train_epoch_loss1=0.6250752819345352 eval_ppl1=tensor(1.8284, device='cuda:0') eval_epoch_loss1=tensor(0.6034, device='cuda:0')\n",
      "epoch=7: train_ppl2=tensor(1.9851) train_epoch_loss2=0.6856651794403157 eval_ppl2=tensor(1.8284, device='cuda:0') eval_epoch_loss2=tensor(0.6034, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl1=tensor(1.8115) train_epoch_loss1=0.5941795129725274 eval_ppl1=tensor(2.0138, device='cuda:0') eval_epoch_loss1=tensor(0.7000, device='cuda:0')\n",
      "epoch=8: train_ppl2=tensor(1.9621) train_epoch_loss2=0.6740230905248764 eval_ppl2=tensor(2.0138, device='cuda:0') eval_epoch_loss2=tensor(0.7000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl1=tensor(1.8344) train_epoch_loss1=0.6067064423510369 eval_ppl1=tensor(1.7610, device='cuda:0') eval_epoch_loss1=tensor(0.5659, device='cuda:0')\n",
      "epoch=9: train_ppl2=tensor(1.9641) train_epoch_loss2=0.6750155509786403 eval_ppl2=tensor(1.7610, device='cuda:0') eval_epoch_loss2=tensor(0.5659, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10: train_ppl1=tensor(1.7520) train_epoch_loss1=0.560776171532083 eval_ppl1=tensor(1.6775, device='cuda:0') eval_epoch_loss1=tensor(0.5173, device='cuda:0')\n",
      "epoch=10: train_ppl2=tensor(1.8872) train_epoch_loss2=0.6351126178782037 eval_ppl2=tensor(1.6775, device='cuda:0') eval_epoch_loss2=tensor(0.5173, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=11: train_ppl1=tensor(1.7816) train_epoch_loss1=0.5775342479031137 eval_ppl1=tensor(1.6919, device='cuda:0') eval_epoch_loss1=tensor(0.5259, device='cuda:0')\n",
      "epoch=11: train_ppl2=tensor(1.8824) train_epoch_loss2=0.6325623164785669 eval_ppl2=tensor(1.6919, device='cuda:0') eval_epoch_loss2=tensor(0.5259, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=12: train_ppl1=tensor(1.6795) train_epoch_loss1=0.5184765863925853 eval_ppl1=tensor(1.8898, device='cuda:0') eval_epoch_loss1=tensor(0.6365, device='cuda:0')\n",
      "epoch=12: train_ppl2=tensor(1.7593) train_epoch_loss2=0.5648921322315297 eval_ppl2=tensor(1.8898, device='cuda:0') eval_epoch_loss2=tensor(0.6365, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13: train_ppl1=tensor(1.7697) train_epoch_loss1=0.5707907537196545 eval_ppl1=tensor(1.7035, device='cuda:0') eval_epoch_loss1=tensor(0.5327, device='cuda:0')\n",
      "epoch=13: train_ppl2=tensor(1.8536) train_epoch_loss2=0.6171531150949762 eval_ppl2=tensor(1.7035, device='cuda:0') eval_epoch_loss2=tensor(0.5327, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=14: train_ppl1=tensor(1.6630) train_epoch_loss1=0.5086020503906493 eval_ppl1=tensor(1.7169, device='cuda:0') eval_epoch_loss1=tensor(0.5405, device='cuda:0')\n",
      "epoch=14: train_ppl2=tensor(1.7206) train_epoch_loss2=0.5426517277956009 eval_ppl2=tensor(1.7169, device='cuda:0') eval_epoch_loss2=tensor(0.5405, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=15: train_ppl1=tensor(1.6886) train_epoch_loss1=0.5239019977285507 eval_ppl1=tensor(1.9444, device='cuda:0') eval_epoch_loss1=tensor(0.6650, device='cuda:0')\n",
      "epoch=15: train_ppl2=tensor(1.7554) train_epoch_loss2=0.562713030170887 eval_ppl2=tensor(1.9444, device='cuda:0') eval_epoch_loss2=tensor(0.6650, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=16: train_ppl1=tensor(1.6773) train_epoch_loss1=0.5171648029317247 eval_ppl1=tensor(1.7441, device='cuda:0') eval_epoch_loss1=tensor(0.5562, device='cuda:0')\n",
      "epoch=16: train_ppl2=tensor(1.7110) train_epoch_loss2=0.5370709442711891 eval_ppl2=tensor(1.7441, device='cuda:0') eval_epoch_loss2=tensor(0.5562, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=17: train_ppl1=tensor(1.6421) train_epoch_loss1=0.4959743663351587 eval_ppl1=tensor(2.6210, device='cuda:0') eval_epoch_loss1=tensor(0.9636, device='cuda:0')\n",
      "epoch=17: train_ppl2=tensor(1.7115) train_epoch_loss2=0.5373579485619322 eval_ppl2=tensor(2.6210, device='cuda:0') eval_epoch_loss2=tensor(0.9636, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=18: train_ppl1=tensor(1.7066) train_epoch_loss1=0.5345122514570013 eval_ppl1=tensor(1.7479, device='cuda:0') eval_epoch_loss1=tensor(0.5584, device='cuda:0')\n",
      "epoch=18: train_ppl2=tensor(1.8010) train_epoch_loss2=0.5883679757726953 eval_ppl2=tensor(1.7479, device='cuda:0') eval_epoch_loss2=tensor(0.5584, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=19: train_ppl1=tensor(1.6250) train_epoch_loss1=0.48551370552245604 eval_ppl1=tensor(1.5999, device='cuda:0') eval_epoch_loss1=tensor(0.4700, device='cuda:0')\n",
      "epoch=19: train_ppl2=tensor(1.7548) train_epoch_loss2=0.5623709220201412 eval_ppl2=tensor(1.5999, device='cuda:0') eval_epoch_loss2=tensor(0.4700, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=20: train_ppl1=tensor(1.6019) train_epoch_loss1=0.4712043214351573 eval_ppl1=tensor(1.6188, device='cuda:0') eval_epoch_loss1=tensor(0.4817, device='cuda:0')\n",
      "epoch=20: train_ppl2=tensor(1.7277) train_epoch_loss2=0.5468009386924987 eval_ppl2=tensor(1.6188, device='cuda:0') eval_epoch_loss2=tensor(0.4817, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=21: train_ppl1=tensor(1.6210) train_epoch_loss1=0.48305965896616593 eval_ppl1=tensor(1.6006, device='cuda:0') eval_epoch_loss1=tensor(0.4704, device='cuda:0')\n",
      "epoch=21: train_ppl2=tensor(1.7111) train_epoch_loss2=0.5371512339470235 eval_ppl2=tensor(1.6006, device='cuda:0') eval_epoch_loss2=tensor(0.4704, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=22: train_ppl1=tensor(1.5999) train_epoch_loss1=0.46992340049845105 eval_ppl1=tensor(1.9137, device='cuda:0') eval_epoch_loss1=tensor(0.6491, device='cuda:0')\n",
      "epoch=22: train_ppl2=tensor(1.6312) train_epoch_loss2=0.4892870247998136 eval_ppl2=tensor(1.9137, device='cuda:0') eval_epoch_loss2=tensor(0.6491, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=23: train_ppl1=tensor(1.5881) train_epoch_loss1=0.46252176482626733 eval_ppl1=tensor(1.7886, device='cuda:0') eval_epoch_loss1=tensor(0.5815, device='cuda:0')\n",
      "epoch=23: train_ppl2=tensor(1.6538) train_epoch_loss2=0.5030808921190019 eval_ppl2=tensor(1.7886, device='cuda:0') eval_epoch_loss2=tensor(0.5815, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=24: train_ppl1=tensor(1.6402) train_epoch_loss1=0.4948237199098506 eval_ppl1=tensor(1.6999, device='cuda:0') eval_epoch_loss1=tensor(0.5306, device='cuda:0')\n",
      "epoch=24: train_ppl2=tensor(1.6639) train_epoch_loss2=0.5091466916368362 eval_ppl2=tensor(1.6999, device='cuda:0') eval_epoch_loss2=tensor(0.5306, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=25: train_ppl1=tensor(1.5790) train_epoch_loss1=0.4567798807265911 eval_ppl1=tensor(1.9751, device='cuda:0') eval_epoch_loss1=tensor(0.6806, device='cuda:0')\n",
      "epoch=25: train_ppl2=tensor(1.6197) train_epoch_loss2=0.48225450642565465 eval_ppl2=tensor(1.9751, device='cuda:0') eval_epoch_loss2=tensor(0.6806, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=26: train_ppl1=tensor(1.5824) train_epoch_loss1=0.4589606020995911 eval_ppl1=tensor(1.7248, device='cuda:0') eval_epoch_loss1=tensor(0.5451, device='cuda:0')\n",
      "epoch=26: train_ppl2=tensor(1.6091) train_epoch_loss2=0.47567588503056385 eval_ppl2=tensor(1.7248, device='cuda:0') eval_epoch_loss2=tensor(0.5451, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=27: train_ppl1=tensor(1.5623) train_epoch_loss1=0.44619008930439646 eval_ppl1=tensor(2.0165, device='cuda:0') eval_epoch_loss1=tensor(0.7014, device='cuda:0')\n",
      "epoch=27: train_ppl2=tensor(1.5634) train_epoch_loss2=0.4468906360103729 eval_ppl2=tensor(2.0165, device='cuda:0') eval_epoch_loss2=tensor(0.7014, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=28: train_ppl1=tensor(1.5474) train_epoch_loss1=0.4366070802541489 eval_ppl1=tensor(1.6099, device='cuda:0') eval_epoch_loss1=tensor(0.4762, device='cuda:0')\n",
      "epoch=28: train_ppl2=tensor(1.5983) train_epoch_loss2=0.4689374753135316 eval_ppl2=tensor(1.6099, device='cuda:0') eval_epoch_loss2=tensor(0.4762, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=29: train_ppl1=tensor(1.5286) train_epoch_loss1=0.42434035305013046 eval_ppl1=tensor(1.7674, device='cuda:0') eval_epoch_loss1=tensor(0.5695, device='cuda:0')\n",
      "epoch=29: train_ppl2=tensor(1.5894) train_epoch_loss2=0.46336897668686317 eval_ppl2=tensor(1.7674, device='cuda:0') eval_epoch_loss2=tensor(0.5695, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=30: train_ppl1=tensor(1.5053) train_epoch_loss1=0.40899323545237803 eval_ppl1=tensor(1.6623, device='cuda:0') eval_epoch_loss1=tensor(0.5082, device='cuda:0')\n",
      "epoch=30: train_ppl2=tensor(1.5402) train_epoch_loss2=0.4319391263292191 eval_ppl2=tensor(1.6623, device='cuda:0') eval_epoch_loss2=tensor(0.5082, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  1.99s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=31: train_ppl1=tensor(1.5222) train_epoch_loss1=0.4201381304796706 eval_ppl1=tensor(1.5705, device='cuda:0') eval_epoch_loss1=tensor(0.4514, device='cuda:0')\n",
      "epoch=31: train_ppl2=tensor(1.5458) train_epoch_loss2=0.4355688751377958 eval_ppl2=tensor(1.5705, device='cuda:0') eval_epoch_loss2=tensor(0.4514, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=32: train_ppl1=tensor(1.5075) train_epoch_loss1=0.4104286311788762 eval_ppl1=tensor(1.9854, device='cuda:0') eval_epoch_loss1=tensor(0.6858, device='cuda:0')\n",
      "epoch=32: train_ppl2=tensor(1.5554) train_epoch_loss2=0.4417516639892091 eval_ppl2=tensor(1.9854, device='cuda:0') eval_epoch_loss2=tensor(0.6858, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=33: train_ppl1=tensor(1.5313) train_epoch_loss1=0.4261247380933863 eval_ppl1=tensor(1.5749, device='cuda:0') eval_epoch_loss1=tensor(0.4542, device='cuda:0')\n",
      "epoch=33: train_ppl2=tensor(1.5210) train_epoch_loss2=0.41933769241292423 eval_ppl2=tensor(1.5749, device='cuda:0') eval_epoch_loss2=tensor(0.4542, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=34: train_ppl1=tensor(1.5081) train_epoch_loss1=0.4108523907179528 eval_ppl1=tensor(1.5412, device='cuda:0') eval_epoch_loss1=tensor(0.4326, device='cuda:0')\n",
      "epoch=34: train_ppl2=tensor(1.5505) train_epoch_loss2=0.4385620095628373 eval_ppl2=tensor(1.5412, device='cuda:0') eval_epoch_loss2=tensor(0.4326, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=35: train_ppl1=tensor(1.5242) train_epoch_loss1=0.4214511640528415 eval_ppl1=tensor(1.7297, device='cuda:0') eval_epoch_loss1=tensor(0.5480, device='cuda:0')\n",
      "epoch=35: train_ppl2=tensor(1.5589) train_epoch_loss2=0.4439897500771157 eval_ppl2=tensor(1.7297, device='cuda:0') eval_epoch_loss2=tensor(0.5480, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=36: train_ppl1=tensor(1.5115) train_epoch_loss1=0.4131002827210629 eval_ppl1=tensor(1.5829, device='cuda:0') eval_epoch_loss1=tensor(0.4592, device='cuda:0')\n",
      "epoch=36: train_ppl2=tensor(1.5318) train_epoch_loss2=0.4264619263245704 eval_ppl2=tensor(1.5829, device='cuda:0') eval_epoch_loss2=tensor(0.4592, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=37: train_ppl1=tensor(1.5015) train_epoch_loss1=0.4064508151817829 eval_ppl1=tensor(1.6733, device='cuda:0') eval_epoch_loss1=tensor(0.5148, device='cuda:0')\n",
      "epoch=37: train_ppl2=tensor(1.5406) train_epoch_loss2=0.43219514000923076 eval_ppl2=tensor(1.6733, device='cuda:0') eval_epoch_loss2=tensor(0.5148, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=38: train_ppl1=tensor(1.4219) train_epoch_loss1=0.3519665732028637 eval_ppl1=tensor(1.7687, device='cuda:0') eval_epoch_loss1=tensor(0.5702, device='cuda:0')\n",
      "epoch=38: train_ppl2=tensor(1.4550) train_epoch_loss2=0.3750063607350309 eval_ppl2=tensor(1.7687, device='cuda:0') eval_epoch_loss2=tensor(0.5702, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=39: train_ppl1=tensor(1.4624) train_epoch_loss1=0.3800735170061284 eval_ppl1=tensor(1.8275, device='cuda:0') eval_epoch_loss1=tensor(0.6030, device='cuda:0')\n",
      "epoch=39: train_ppl2=tensor(1.4745) train_epoch_loss2=0.3883132368643233 eval_ppl2=tensor(1.8275, device='cuda:0') eval_epoch_loss2=tensor(0.6030, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=40: train_ppl1=tensor(1.4157) train_epoch_loss1=0.3476271967780083 eval_ppl1=tensor(2.0295, device='cuda:0') eval_epoch_loss1=tensor(0.7078, device='cuda:0')\n",
      "epoch=40: train_ppl2=tensor(1.4462) train_epoch_loss2=0.36894043796557063 eval_ppl2=tensor(2.0295, device='cuda:0') eval_epoch_loss2=tensor(0.7078, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=41: train_ppl1=tensor(1.4234) train_epoch_loss1=0.35302260486369436 eval_ppl1=tensor(1.6415, device='cuda:0') eval_epoch_loss1=tensor(0.4956, device='cuda:0')\n",
      "epoch=41: train_ppl2=tensor(1.4987) train_epoch_loss2=0.40462351780622563 eval_ppl2=tensor(1.6415, device='cuda:0') eval_epoch_loss2=tensor(0.4956, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=42: train_ppl1=tensor(1.4463) train_epoch_loss1=0.36899209196897265 eval_ppl1=tensor(1.7576, device='cuda:0') eval_epoch_loss1=tensor(0.5639, device='cuda:0')\n",
      "epoch=42: train_ppl2=tensor(1.4590) train_epoch_loss2=0.3777786429892195 eval_ppl2=tensor(1.7576, device='cuda:0') eval_epoch_loss2=tensor(0.5639, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=43: train_ppl1=tensor(1.4128) train_epoch_loss1=0.3455392846718748 eval_ppl1=tensor(1.6727, device='cuda:0') eval_epoch_loss1=tensor(0.5144, device='cuda:0')\n",
      "epoch=43: train_ppl2=tensor(1.4148) train_epoch_loss2=0.34700676640297506 eval_ppl2=tensor(1.6727, device='cuda:0') eval_epoch_loss2=tensor(0.5144, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=44: train_ppl1=tensor(1.4375) train_epoch_loss1=0.3629383714275157 eval_ppl1=tensor(1.5776, device='cuda:0') eval_epoch_loss1=tensor(0.4559, device='cuda:0')\n",
      "epoch=44: train_ppl2=tensor(1.4326) train_epoch_loss2=0.35952148095090336 eval_ppl2=tensor(1.5776, device='cuda:0') eval_epoch_loss2=tensor(0.4559, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=45: train_ppl1=tensor(1.4154) train_epoch_loss1=0.34741641977365983 eval_ppl1=tensor(1.6622, device='cuda:0') eval_epoch_loss1=tensor(0.5081, device='cuda:0')\n",
      "epoch=45: train_ppl2=tensor(1.4590) train_epoch_loss2=0.3777644537547801 eval_ppl2=tensor(1.6622, device='cuda:0') eval_epoch_loss2=tensor(0.5081, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=46: train_ppl1=tensor(1.3604) train_epoch_loss1=0.30776884636663376 eval_ppl1=tensor(1.5937, device='cuda:0') eval_epoch_loss1=tensor(0.4661, device='cuda:0')\n",
      "epoch=46: train_ppl2=tensor(1.4006) train_epoch_loss2=0.33687676917365256 eval_ppl2=tensor(1.5937, device='cuda:0') eval_epoch_loss2=tensor(0.4661, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=47: train_ppl1=tensor(1.3780) train_epoch_loss1=0.32066789824277797 eval_ppl1=tensor(1.6601, device='cuda:0') eval_epoch_loss1=tensor(0.5069, device='cuda:0')\n",
      "epoch=47: train_ppl2=tensor(1.4167) train_epoch_loss2=0.3483059081308385 eval_ppl2=tensor(1.6601, device='cuda:0') eval_epoch_loss2=tensor(0.5069, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=48: train_ppl1=tensor(1.3953) train_epoch_loss1=0.3330843042027443 eval_ppl1=tensor(1.5945, device='cuda:0') eval_epoch_loss1=tensor(0.4665, device='cuda:0')\n",
      "epoch=48: train_ppl2=tensor(1.4254) train_epoch_loss2=0.35447386065696146 eval_ppl2=tensor(1.5945, device='cuda:0') eval_epoch_loss2=tensor(0.4665, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:33<00:00,  2.00s/it]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.27it/s]\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=49: train_ppl1=tensor(1.3848) train_epoch_loss1=0.3255284591082563 eval_ppl1=tensor(1.5999, device='cuda:0') eval_epoch_loss1=tensor(0.4699, device='cuda:0')\n",
      "epoch=49: train_ppl2=tensor(1.4135) train_epoch_loss2=0.3460675330555185 eval_ppl2=tensor(1.5999, device='cuda:0') eval_epoch_loss2=tensor(0.4699, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:25<00:00,  1.25it/s]\n",
      "100%|██████████| 32/32 [00:25<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training1: init_test_ppl1=tensor(1.5130) init_test_loss1=tensor(0.4141)\n",
      "Test before training2: init_test_ppl2=tensor(1.5130) init_test_loss2=tensor(0.4141)\n",
      "Test after training1: final_test_ppl1=tensor(1.2252) final_test_loss1=tensor(0.2031)\n",
      "Test after training2: final_test_ppl2=tensor(1.2252) final_test_loss2=tensor(0.2031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    MultitaskPromptTuningConfig,\n",
    "    MultitaskPromptTuningInit,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name_or_path = \"microsoft/phi-2\"\n",
    "tokenizer_name_or_path = \"microsoft/phi-2\"\n",
    "\n",
    "dataset_id = \"navigate\"\n",
    "initial_instruction = (\n",
    "    \"Read the following question, then choose the correct answer.\"\n",
    ")\n",
    "text_column = \"text\"\n",
    "label_column = \"label\"\n",
    "max_length = 128\n",
    "lr = 3e-2\n",
    "num_epochs = 50\n",
    "batch_size = 8\n",
    "\n",
    "peft_config = MultitaskPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_tasks=2,\n",
    "    prompt_tuning_init=MultitaskPromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    num_transformer_submodules=1,\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "dataset = load_dln_dataset_to_hf_dataset(dataset_id)\n",
    "\n",
    "classes = list(set(dataset[\"train\"][\"label\"]))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, device_map=\"auto\", padding_side='left')\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max(\n",
    "    [len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes]\n",
    ")\n",
    "print(target_max_length)\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"prefix\": '',\n",
    "        \"text_column\": text_column,\n",
    "        \"label_column\": label_column,\n",
    "        \"max_length\": max_length,\n",
    "    },\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"dev\"]\n",
    "test_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "global model\n",
    "if saved_model1 is None or saved_model2 is None:\n",
    "    if model is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model1 = get_peft_model(model, peft_config)\n",
    "    model2 = get_peft_model(model, peft_config)\n",
    "else:\n",
    "    model1 = saved_model1\n",
    "    model2 = saved_model2\n",
    "    print(\"Using saved model from data/models/\" + model_name_or_path)\n",
    "    \n",
    "optimizer1 = torch.optim.AdamW(model1.parameters(), lr=lr)\n",
    "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=lr)\n",
    "lr_scheduler1 = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer1,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "\n",
    ")\n",
    "lr_scheduler2 = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer2,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "model1 = model1.to(device)\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "init_test_loss1, test_preds1 = test(test_dataloader, model1, model2, tokenizer, device)\n",
    "init_test_loss2, test_preds2 = test(test_dataloader, model1, model2, tokenizer, device)\n",
    "init_test_ppl1 = torch.exp(init_test_loss1)  # Perplexity\n",
    "init_test_ppl2 = torch.exp(init_test_loss2)  # Perplexity\n",
    "print(f\"Test before training1: {init_test_ppl1=} {init_test_loss1=}\")\n",
    "print(f\"Test before training2: {init_test_ppl2=} {init_test_loss2=}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss1 = 0\n",
    "    total_loss2 = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        output1 = model1(**batch, output_hidden_states=True)\n",
    "        # print(\"1=\" + tokenizer.batch_decode(torch.argmax(output1.logits, dim=-1)[-1])[-1])\n",
    "        \n",
    "        inputs_embeds = output1.hidden_states[-1]\n",
    "        sequence_length = inputs_embeds.shape[1]\n",
    "        labels = batch['labels']\n",
    "        attention_mask = torch.ones(inputs_embeds.shape[:2], device=device)\n",
    "        padding = torch.full((labels.shape[0], sequence_length - labels.shape[1]), -100, dtype=labels.dtype, device=labels.device)\n",
    "        labels = torch.cat([padding, labels], dim=1).to(device)\n",
    "        task_ids = torch.tensor([1 for i in batch[\"task_ids\"]]).to(device)\n",
    "        output2 = model2(inputs_embeds=inputs_embeds, labels=labels, task_ids=task_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        # print(\"2=\" + tokenizer.batch_decode(torch.argmax(output2.logits, dim=-1)[-1])[-1])\n",
    "\n",
    "        loss1 = output1.loss\n",
    "        loss2 = output2.loss\n",
    "        # print (f\"loss: {loss1.item()=}, {loss2.item()=}\")\n",
    "        total_loss1 += loss1.item()\n",
    "        total_loss2 += loss2.item()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        lr_scheduler1.step()\n",
    "        lr_scheduler2.step()\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    eval_epoch_loss1, eval_preds1 = test(eval_dataloader, model1, model2, tokenizer, device, False)\n",
    "    eval_epoch_loss2, eval_preds2 = test(eval_dataloader, model1, model2, tokenizer, device, False)\n",
    "    eval_ppl1 = torch.exp(eval_epoch_loss1)\n",
    "    eval_ppl2 = torch.exp(eval_epoch_loss2)\n",
    "    train_epoch_loss1 = total_loss1 / len(train_dataloader)\n",
    "    train_epoch_loss2 = total_loss2 / len(train_dataloader)\n",
    "    train_ppl1 = torch.exp(torch.tensor(train_epoch_loss1))\n",
    "    train_ppl2 = torch.exp(torch.tensor(train_epoch_loss2))\n",
    "    print(\n",
    "        f\"{epoch=}: {train_ppl1=} {train_epoch_loss1=} {eval_ppl1=} {eval_epoch_loss1=}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{epoch=}: {train_ppl2=} {train_epoch_loss2=} {eval_ppl2=} {eval_epoch_loss2=}\"\n",
    "    )\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "if not saved_model1:\n",
    "    model1.save_pretrained(\"data/models/\" + model_name_or_path + \"/model1\")\n",
    "if not saved_model2:\n",
    "    model2.save_pretrained(\"data/models/\" + model_name_or_path + \"/model2\")\n",
    "\n",
    "final_test_loss1, test_preds1 = test(test_dataloader, model1, model2, tokenizer, device)\n",
    "final_test_loss2, test_preds2 = test(test_dataloader, model1, model2, tokenizer, device)\n",
    "final_test_ppl1 = torch.exp(final_test_loss1)\n",
    "final_test_ppl2 = torch.exp(final_test_loss2)\n",
    "print(f\"Test before training1: {init_test_ppl1=} {init_test_loss1=}\")\n",
    "print(f\"Test before training2: {init_test_ppl2=} {init_test_loss2=}\")\n",
    "print(f\"Test after training1: {final_test_ppl1=} {final_test_loss1=}\")\n",
    "print(f\"Test after training2: {final_test_ppl2=} {final_test_loss2=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f7411",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74fa6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=79.2% on the test dataset\n",
      "test_preds1[:10]=['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
      "dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for pred, label in zip(test_preds1,  dataset['test']['label']):\n",
    "    if pred.strip() == label.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"{accuracy=}% on the test dataset\")\n",
    "print(f\"{test_preds1[:10]=}\")\n",
    "print(f\"{dataset['test']['label'][:10]=}\")\n",
    "\n",
    "\"accuracy=79.2% on the test dataset\"\n",
    "\"test_preds[:10]=['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\"\n",
    "\"dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cde8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=79.2% on the test dataset\n",
      "test_preds2[:10]=['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
      "dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for pred, label in zip(test_preds2,  dataset['test']['label']):\n",
    "    if pred.strip() == label.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"{accuracy=}% on the test dataset\")\n",
    "print(f\"{test_preds2[:10]=}\")\n",
    "print(f\"{dataset['test']['label'][:10]=}\")\n",
    "\n",
    "\"accuracy=79.2% on the test dataset\"\n",
    "\"test_preds[:10]=['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\"\n",
    "\"dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
