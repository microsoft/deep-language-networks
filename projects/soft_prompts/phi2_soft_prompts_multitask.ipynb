{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b64723f-a113-466d-8057-920ce0d062c3",
   "metadata": {},
   "source": [
    "## Multitask prompt tuning using Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1ccb88-ca67-4f86-b40e-506a5d4aac3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chsingh\\source\\repos\\deep-language-networks\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe8255e-66e2-450d-bf05-f3a6183719b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.53s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the following sentence, then determine whether you return to the starting point.\n",
      "\n",
      "If you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "\n",
      "Answer:\n",
      "To solve this question, we need to keep track of the number of steps taken and the direction of each turn.\n",
      "\n",
      "Starting from the initial position, we take 9 steps forward. Then, we take another 9 steps forward. Next, we take 4 steps forward. Finally, we turn right.\n",
      "\n",
      "Since we have taken a total of 9 + 9 + 4 = 22 steps and turned right, we do not return to the starting point.\n",
      "\n",
      "\n",
      "Complete detailed textbook-level python code solutions\n",
      "```python\n",
      "# Initialize variables\n",
      "steps_taken = 0\n",
      "direction = 0  # 0: North, 1: East, 2: South, 3: West\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 9 steps forward\n",
      "steps_taken += 9\n",
      "\n",
      "# Take 4 steps forward\n",
      "steps_taken += 4\n",
      "\n",
      "# Turn right\n",
      "direction = (direction + 1) % 4\n",
      "\n",
      "# Check if returned to starting point\n",
      "if steps_taken == 0 and direction == 0:\n",
      "    print(\"Yes\")\n",
      "else:\n",
      "    print(\"No\")\n",
      "\n",
      "Using saved model from data/models/microsoft/phi-2\n",
      "Model not found, training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chsingh\\source\\repos\\deep-language-networks\\.venv\\lib\\site-packages\\peft\\peft_model.py:1232: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    MultitaskPromptTuningConfig,\n",
    "    MultitaskPromptTuningInit,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "initial_instruction = (\n",
    "    \"Read the following question, then choose the correction answer.\"\n",
    ")\n",
    "\n",
    "peft_config = MultitaskPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_tasks=2,\n",
    "    prompt_tuning_init=MultitaskPromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    num_transformer_submodules=1,\n",
    "    tokenizer_name_or_path=model_id,\n",
    ")\n",
    "\n",
    "model = None\n",
    "saved_model1 = None\n",
    "saved_model2 = None\n",
    "\n",
    "try:\n",
    "    sentences = [\"Read the following sentence, then determine whether you return to the starting point.\\n\\nIf you follow these instructions, do you return to the starting point? Take 9 steps. Take 9 steps. Take 4 steps. Turn right.\\nOptions:\\n- Yes\\n- No\\n\\nAnswer:\\n\"]\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model.to(device)\n",
    "    generate_ids = model.generate(**inputs, max_length=500)\n",
    "    outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(outputs[0])\n",
    "\n",
    "    print(\"Using saved model from data/models/\" + model_id)\n",
    "    saved_model1 = PeftModel.from_pretrained(model, \"data/models/\" + model_id + \"/model1\")\n",
    "    saved_model2 = PeftModel.from_pretrained(model, \"data/models/\" + model_id + \"/model2\")\n",
    "    saved_model1.to(device)\n",
    "    saved_model2.to(device)\n",
    "    generate_ids1 = saved_model1.generate(**inputs, max_length=500)\n",
    "    generate_ids2 = saved_model2.generate(**inputs, max_length=500)\n",
    "    outputs1 = tokenizer.batch_decode(generate_ids1, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    outputs2 = tokenizer.batch_decode(generate_ids2, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(outputs1[0])\n",
    "    print(outputs2[0])\n",
    "except ValueError:\n",
    "    print(\"Model not found, training new model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9566ed-03c7-4581-9928-8f140d095c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, prefix, text_column, label_column, max_length):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{prefix}{x}\\n\\nAnswer:\\n\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=max_length)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Replace padding tokens in the labels with -100\n",
    "    labels[\"input_ids\"] = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels[\"input_ids\"]]\n",
    "\n",
    "    task_ids = [0 for i in labels[\"input_ids\"]]\n",
    "    task_ids = torch.tensor(task_ids)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"task_ids\"] = task_ids\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6704297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobs_for_classes(output_logits, classes):\n",
    "    logits = [0 for _ in range(len(classes))]\n",
    "    for i, target in enumerate(classes):\n",
    "        expanded_classes = [target] + [f\" {target}\"] + [f\"{target.lower()}\"] + [f\" {target.lower()}\"]\n",
    "        encoded_classes = [tokenizer.encode(c, return_tensors=\"pt\", padding=True).to(device) for c in expanded_classes]\n",
    "        for token in encoded_classes:\n",
    "            logits[i] += output_logits[token]\n",
    "    return F.log_softmax(torch.tensor(logits), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c28c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_loss(outputs, labels):     \n",
    "    target_texts = [tokenizer.decode([tok for tok in target if tok != -100], skip_special_tokens=True) for target in labels]\n",
    "    targets = list(set(target_texts))\n",
    "    generated_texts = [targets[np.argmax(logprobs_for_classes(out[-1], targets))] for out in outputs.logits]        \n",
    "\n",
    "    losses = []\n",
    "    for generated_text, target_text in zip(generated_texts, target_texts):\n",
    "        generated_tokens = generated_text.split()\n",
    "        target_tokens = target_text.split()\n",
    "        loss = sum(generated_token != target_token for generated_token, target_token in zip(generated_tokens, target_tokens))\n",
    "        losses.append(loss)\n",
    "\n",
    "    loss_tensor = torch.tensor(losses, dtype=torch.float32)\n",
    "    total_loss = torch.mean(loss_tensor)\n",
    "    return total_loss, generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c086d1b8-0f20-40ae-889c-d81155c9bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, tokenizer, device, exact_match=True):\n",
    "    total_loss = 0\n",
    "    test_preds = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss, preds = exact_match_loss(outputs, batch[\"labels\"]) if exact_match else (outputs.loss, [])\n",
    "        total_loss += loss.detach().float()\n",
    "        test_preds.extend(preds)\n",
    "\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_loss, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d765ea45-01e5-47d1-bca3-87a6929738db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dln.dataset import init_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def load_dln_dataset_to_hf_dataset(dataset_id):\n",
    "    \"\"\"Some gynmastics to load the dln dataset into a HuggingFace Dataset.\n",
    "    dln.dataset should implement an interface compatible with HuggingFace\"\"\"\n",
    "\n",
    "    dln_dataset = init_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        seed=42,\n",
    "        data_dir=os.path.dirname(os.getcwd()) + \"/../data\",\n",
    "    )\n",
    "\n",
    "    def load_split(split):\n",
    "        text_data, label_data = dln_dataset.get_data(split)\n",
    "        data_dict = {\"text\": text_data, \"label\": label_data}\n",
    "        dataset = Dataset.from_dict(data_dict, split=split)\n",
    "        return dataset\n",
    "\n",
    "    # Combine the datasets into a DatasetDict\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": load_split(\"train\"),\n",
    "            \"dev\": load_split(\"dev\"),\n",
    "            \"test\": load_split(\"test\"),\n",
    "        }\n",
    "    )\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443a5190-5204-495e-bc5e-420857e89c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset from c:\\Users\\chsingh\\source\\repos\\deep-language-networks\\projects/../data\\bbh ...\n",
      "we have 375 training, 375 dev, and 250 test data points.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/375 [00:00<?, ? examples/s]c:\\Users\\chsingh\\source\\repos\\deep-language-networks\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 1942.91 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 375/375 [00:00<00:00, 1314.64 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 250/250 [00:00<00:00, 2096.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved model from data/models/microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:16<00:00,  3.75it/s]\n",
      "100%|██████████| 63/63 [00:16<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training1: init_test_ppl1=tensor(1.5783) init_test_loss1=tensor(0.4563)\n",
      "Test before training2: init_test_ppl2=tensor(1.5783) init_test_loss2=tensor(0.4563)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [01:38<00:00,  1.05s/it]\n",
      "100%|██████████| 94/94 [00:19<00:00,  4.80it/s]\n",
      "100%|██████████| 94/94 [00:19<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl1=tensor(15.7864) train_epoch_loss1=2.7591474474744593 eval_ppl1=tensor(2.0827, device='cuda:0') eval_epoch_loss1=tensor(0.7336, device='cuda:0')\n",
      "epoch=0: train_ppl2=tensor(4.6689) train_epoch_loss2=1.5409164234916581 eval_ppl2=tensor(67510.4766, device='cuda:0') eval_epoch_loss2=tensor(11.1200, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:59<00:00,  2.54s/it]\n",
      "100%|██████████| 94/94 [02:48<00:00,  1.79s/it]\n",
      "100%|██████████| 94/94 [02:47<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl1=tensor(2.0444) train_epoch_loss1=0.7151273609475887 eval_ppl1=tensor(1.9158, device='cuda:0') eval_epoch_loss1=tensor(0.6501, device='cuda:0')\n",
      "epoch=1: train_ppl2=tensor(1.0980) train_epoch_loss2=0.09349546323906868 eval_ppl2=tensor(67494.9609, device='cuda:0') eval_epoch_loss2=tensor(11.1198, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [08:38<00:00,  5.51s/it]\n",
      "100%|██████████| 94/94 [02:48<00:00,  1.79s/it]\n",
      "100%|██████████| 94/94 [02:47<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl1=tensor(1.9723) train_epoch_loss1=0.67922324766504 eval_ppl1=tensor(1.8484, device='cuda:0') eval_epoch_loss1=tensor(0.6143, device='cuda:0')\n",
      "epoch=2: train_ppl2=tensor(1.0944) train_epoch_loss2=0.09022092930179962 eval_ppl2=tensor(67479.1250, device='cuda:0') eval_epoch_loss2=tensor(11.1196, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [08:38<00:00,  5.51s/it]\n",
      "100%|██████████| 94/94 [02:48<00:00,  1.79s/it]\n",
      "100%|██████████| 94/94 [02:48<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl1=tensor(1.9331) train_epoch_loss1=0.6591103029377917 eval_ppl1=tensor(1.8113, device='cuda:0') eval_epoch_loss1=tensor(0.5940, device='cuda:0')\n",
      "epoch=3: train_ppl2=tensor(1.0935) train_epoch_loss2=0.08940663707858705 eval_ppl2=tensor(67265.7500, device='cuda:0') eval_epoch_loss2=tensor(11.1164, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [08:37<00:00,  5.51s/it]\n",
      "100%|██████████| 94/94 [02:47<00:00,  1.79s/it]\n",
      "100%|██████████| 94/94 [02:47<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl1=tensor(1.8306) train_epoch_loss1=0.6046698470699027 eval_ppl1=tensor(1.7930, device='cuda:0') eval_epoch_loss1=tensor(0.5839, device='cuda:0')\n",
      "epoch=4: train_ppl2=tensor(1.0898) train_epoch_loss2=0.08594910970869217 eval_ppl2=tensor(67257.6641, device='cuda:0') eval_epoch_loss2=tensor(11.1163, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [08:38<00:00,  5.51s/it]\n",
      "100%|██████████| 94/94 [02:47<00:00,  1.78s/it]\n",
      "100%|██████████| 94/94 [02:47<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl1=tensor(1.8501) train_epoch_loss1=0.615257886495996 eval_ppl1=tensor(1.7444, device='cuda:0') eval_epoch_loss1=tensor(0.5564, device='cuda:0')\n",
      "epoch=5: train_ppl2=tensor(1.0870) train_epoch_loss2=0.08343937537296021 eval_ppl2=tensor(67231.2422, device='cuda:0') eval_epoch_loss2=tensor(11.1159, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [08:37<00:00,  5.50s/it]\n",
      "100%|██████████| 94/94 [02:48<00:00,  1.80s/it]\n",
      "100%|██████████| 94/94 [02:56<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl1=tensor(1.7819) train_epoch_loss1=0.5776811990332096 eval_ppl1=tensor(1.6802, device='cuda:0') eval_epoch_loss1=tensor(0.5189, device='cuda:0')\n",
      "epoch=6: train_ppl2=tensor(1.0864) train_epoch_loss2=0.08289719616716847 eval_ppl2=tensor(67202.2734, device='cuda:0') eval_epoch_loss2=tensor(11.1155, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [09:09<00:00,  5.85s/it]\n",
      "100%|██████████| 94/94 [02:46<00:00,  1.77s/it]\n",
      "100%|██████████| 94/94 [03:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl1=tensor(1.7458) train_epoch_loss1=0.5572342584107784 eval_ppl1=tensor(1.7560, device='cuda:0') eval_epoch_loss1=tensor(0.5630, device='cuda:0')\n",
      "epoch=7: train_ppl2=tensor(1.0859) train_epoch_loss2=0.08240805475160162 eval_ppl2=tensor(67240.1562, device='cuda:0') eval_epoch_loss2=tensor(11.1160, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [09:25<00:00,  6.01s/it]\n",
      "100%|██████████| 94/94 [03:03<00:00,  1.95s/it]\n",
      "100%|██████████| 94/94 [03:02<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl1=tensor(1.7323) train_epoch_loss1=0.5494530758959182 eval_ppl1=tensor(1.7612, device='cuda:0') eval_epoch_loss1=tensor(0.5660, device='cuda:0')\n",
      "epoch=8: train_ppl2=tensor(1.0833) train_epoch_loss2=0.07999642974043146 eval_ppl2=tensor(67196.2500, device='cuda:0') eval_epoch_loss2=tensor(11.1154, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [13:05<00:00,  8.36s/it]\n",
      "100%|██████████| 94/94 [04:10<00:00,  2.66s/it]\n",
      "100%|██████████| 94/94 [03:57<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl1=tensor(1.7063) train_epoch_loss1=0.5343095179884991 eval_ppl1=tensor(1.7515, device='cuda:0') eval_epoch_loss1=tensor(0.5604, device='cuda:0')\n",
      "epoch=9: train_ppl2=tensor(1.0834) train_epoch_loss2=0.08012474847442293 eval_ppl2=tensor(67184.0703, device='cuda:0') eval_epoch_loss2=tensor(11.1152, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:42<00:00,  2.58s/it]\n",
      "100%|██████████| 63/63 [02:42<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training1: init_test_ppl1=tensor(1.5783) init_test_loss1=tensor(0.4563)\n",
      "Test before training2: init_test_ppl2=tensor(1.5783) init_test_loss2=tensor(0.4563)\n",
      "Test after training1: final_test_ppl1=tensor(1.2688) final_test_loss1=tensor(0.2381)\n",
      "Test after training2: final_test_ppl2=tensor(1.5783) final_test_loss2=tensor(0.4563)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    MultitaskPromptTuningConfig,\n",
    "    MultitaskPromptTuningInit,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name_or_path = \"microsoft/phi-2\"\n",
    "tokenizer_name_or_path = \"microsoft/phi-2\"\n",
    "\n",
    "dataset_id = \"navigate\"\n",
    "initial_instruction = (\n",
    "    \"Read the following question, then choose the correct answer.\"\n",
    ")\n",
    "text_column = \"text\"\n",
    "label_column = \"label\"\n",
    "max_length = 128\n",
    "lr = 3e-2\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "\n",
    "peft_config = MultitaskPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_tasks=2,\n",
    "    prompt_tuning_init=MultitaskPromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=initial_instruction,\n",
    "    num_transformer_submodules=1,\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "dataset = load_dln_dataset_to_hf_dataset(dataset_id)\n",
    "\n",
    "classes = list(set(dataset[\"train\"][\"label\"]))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, device_map=\"auto\", padding_side='left')\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max(\n",
    "    [len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes]\n",
    ")\n",
    "print(target_max_length)\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"prefix\": '',\n",
    "        \"text_column\": text_column,\n",
    "        \"label_column\": label_column,\n",
    "        \"max_length\": max_length,\n",
    "    },\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"dev\"]\n",
    "test_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "global model\n",
    "if saved_model1 is None or saved_model2 is None:\n",
    "    if model is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model1 = get_peft_model(model, peft_config)\n",
    "    model2 = get_peft_model(model, peft_config)\n",
    "else:\n",
    "    model1 = saved_model1\n",
    "    model2 = saved_model2\n",
    "    print(\"Using saved model from data/models/\" + model_name_or_path)\n",
    "    \n",
    "optimizer1 = torch.optim.AdamW(model1.parameters(), lr=lr)\n",
    "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=lr)\n",
    "lr_scheduler1 = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer1,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "\n",
    ")\n",
    "lr_scheduler2 = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer2,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "model1 = model1.to(device)\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "init_test_loss1, test_preds1 = test(test_dataloader, model1, tokenizer, device)\n",
    "init_test_loss2, test_preds2 = test(test_dataloader, model2, tokenizer, device)\n",
    "init_test_ppl1 = torch.exp(init_test_loss1)  # Perplexity\n",
    "init_test_ppl2 = torch.exp(init_test_loss2)  # Perplexity\n",
    "print(f\"Test before training1: {init_test_ppl1=} {init_test_loss1=}\")\n",
    "print(f\"Test before training2: {init_test_ppl2=} {init_test_loss2=}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss1 = 0\n",
    "    total_loss2 = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        output1 = model1(**batch, output_hidden_states=True)\n",
    "\n",
    "        inputs_embeds = output1.hidden_states[-1]\n",
    "        sequence_length = inputs_embeds.shape[1]\n",
    "        labels = batch['labels']\n",
    "        padding = torch.zeros((labels.shape[0], sequence_length - labels.shape[1]), dtype=labels.dtype, device=labels.device)\n",
    "        labels = torch.cat([labels, padding], dim=1).to(device)\n",
    "        task_ids = torch.tensor([1 for i in batch[\"task_ids\"]]).to(device)\n",
    "        output2 = model2(inputs_embeds=inputs_embeds, labels=labels, task_ids=task_ids, output_hidden_states=True)\n",
    "        \n",
    "        loss1 = output1.loss\n",
    "        loss2 = output2.loss\n",
    "        total_loss1 += loss1.item()\n",
    "        total_loss2 += loss2.item()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        lr_scheduler1.step()\n",
    "        lr_scheduler2.step()\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    eval_epoch_loss1, eval_preds1 = test(eval_dataloader, model1, tokenizer, device, False)\n",
    "    eval_epoch_loss2, eval_preds2 = test(eval_dataloader, model2, tokenizer, device, False)\n",
    "    eval_ppl1 = torch.exp(eval_epoch_loss1)\n",
    "    eval_ppl2 = torch.exp(eval_epoch_loss2)\n",
    "    train_epoch_loss1 = total_loss1 / len(train_dataloader)\n",
    "    train_epoch_loss2 = total_loss2 / len(train_dataloader)\n",
    "    train_ppl1 = torch.exp(torch.tensor(train_epoch_loss1))\n",
    "    train_ppl2 = torch.exp(torch.tensor(train_epoch_loss2))\n",
    "    print(\n",
    "        f\"{epoch=}: {train_ppl1=} {train_epoch_loss1=} {eval_ppl1=} {eval_epoch_loss1=}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{epoch=}: {train_ppl2=} {train_epoch_loss2=} {eval_ppl2=} {eval_epoch_loss2=}\"\n",
    "    )\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "if not saved_model1:\n",
    "    model1.save_pretrained(\"data/models/\" + model_name_or_path + \"/model1\")\n",
    "if not saved_model2:\n",
    "    model2.save_pretrained(\"data/models/\" + model_name_or_path + \"/model2\")\n",
    "\n",
    "final_test_loss1, test_preds1 = test(test_dataloader, model1, tokenizer, device)\n",
    "final_test_loss2, test_preds2 = test(test_dataloader, model2, tokenizer, device)\n",
    "final_test_ppl1 = torch.exp(final_test_loss1)\n",
    "final_test_ppl2 = torch.exp(final_test_loss2)\n",
    "print(f\"Test before training1: {init_test_ppl1=} {init_test_loss1=}\")\n",
    "print(f\"Test before training2: {init_test_ppl2=} {init_test_loss2=}\")\n",
    "print(f\"Test after training1: {final_test_ppl1=} {final_test_loss1=}\")\n",
    "print(f\"Test after training2: {final_test_ppl2=} {final_test_loss2=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f7411",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74fa6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=76.0% on the test dataset\n",
      "test_preds1[:10]=['No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No']\n",
      "dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for pred, label in zip(test_preds1,  dataset['test']['label']):\n",
    "    if pred.strip() == label.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"{accuracy=}% on the test dataset\")\n",
    "print(f\"{test_preds1[:10]=}\")\n",
    "print(f\"{dataset['test']['label'][:10]=}\")\n",
    "\n",
    "\"accuracy=80.4% on the test dataset\"\n",
    "\"test_preds[:10]=['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\"\n",
    "\"dataset['test']['label'][:10]=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
